{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers==4.44.2 datasets==3.1.0 torch==2.5.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:18:41.283131Z","iopub.execute_input":"2024-11-25T09:18:41.283625Z","iopub.status.idle":"2024-11-25T09:18:52.503684Z","shell.execute_reply.started":"2024-11-25T09:18:41.283583Z","shell.execute_reply":"2024-11-25T09:18:52.501811Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.44.2 in /opt/conda/lib/python3.10/site-packages (4.44.2)\nRequirement already satisfied: datasets==3.1.0 in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: torch==2.5.0 in /opt/conda/lib/python3.10/site-packages (2.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.44.2) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (4.12.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.44.2) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.44.2) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.5.0) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # Use GPU\n    print(\"GPU is available. Using:\", torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")  # Use CPU\n    print(\"GPU is not available. Using CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:18:52.507198Z","iopub.execute_input":"2024-11-25T09:18:52.507629Z","iopub.status.idle":"2024-11-25T09:18:52.516995Z","shell.execute_reply.started":"2024-11-25T09:18:52.507592Z","shell.execute_reply":"2024-11-25T09:18:52.515503Z"}},"outputs":[{"name":"stdout","text":"GPU is not available. Using CPU.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Load the orginal VoxPopuli dataset\ndatasetO = load_dataset(\"facebook/voxpopuli\", \"sl\", split=\"train\")\nlen(datasetO)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:18:52.519022Z","iopub.execute_input":"2024-11-25T09:18:52.519593Z","iopub.status.idle":"2024-11-25T09:18:53.371071Z","shell.execute_reply.started":"2024-11-25T09:18:52.519527Z","shell.execute_reply":"2024-11-25T09:18:53.369647Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"2099"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Set random seed\ntorch.manual_seed(42)\n\n# Load the VoxPopuli dataset\ndataset = load_dataset(\"facebook/voxpopuli\", \"sl\", split=\"train\", streaming= True)\n\n# Shuffle the dataset\nimport random\nrandom.seed(42)\nshuffled_dataset = list(dataset)\nrandom.shuffle(shuffled_dataset)\n\nprint(\"Sample Data:\", shuffled_dataset[:2])  # Display 2 samples for inspection\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:18:53.374307Z","iopub.execute_input":"2024-11-25T09:18:53.374668Z","iopub.status.idle":"2024-11-25T09:19:32.360158Z","shell.execute_reply.started":"2024-11-25T09:18:53.374635Z","shell.execute_reply":"2024-11-25T09:19:32.358747Z"}},"outputs":[{"name":"stdout","text":"Sample Data: [{'audio_id': '20190918-0900-PLENARY-sl_20190918-19:18:48_3', 'language': 13, 'audio': {'path': 'train_part_0/20190918-0900-PLENARY-sl_20190918-19:18:48_3.wav', 'array': array([ 0.00042725, -0.00488281, -0.00680542, ...,  0.00708008,\n        0.00717163,  0.00378418]), 'sampling_rate': 16000}, 'raw_text': 'Nobena od te in podobnih resolucij, ki jih je Evropski parlament sprejel že zelo veliko, ne more biti zares koristna, če sočasno dopuščamo dejanja, ki jih ta resolucija obsoja, in to so zatiranje demokracije, nespoštovanje pravne države, odrekanje pravic manjšinam in drugačnim, avtokracija, nacionalizem.', 'normalized_text': 'nobena od te in podobnih resolucij ki jih je evropski parlament sprejel že zelo veliko ne more biti zares koristna če sočasno dopuščamo dejanja ki jih ta resolucija obsoja in to so zatiranje demokracije nespoštovanje pravne države odrekanje pravic manjšinam in drugačnim avtokracija nacionalizem.', 'gender': 'female', 'speaker_id': '96911', 'is_gold_transcript': True, 'accent': 'None'}, {'audio_id': '20190211-0900-PLENARY-sl_20190211-18:10:42_7', 'language': 13, 'audio': {'path': 'train_part_0/20190211-0900-PLENARY-sl_20190211-18:10:42_7.wav', 'array': array([-0.00125122, -0.00076294,  0.00085449, ...,  0.00073242,\n        0.00115967,  0.00036621]), 'sampling_rate': 16000}, 'raw_text': 'Vsako leto v Evropski uniji pri črpanju iz evropskih skladov izgubljamo velikanske količine sredstev, zato zgolj pozivanje k transparentnosti ne bo dovolj, če ne bodo temu sledili tudi konkretni ukrepi.', 'normalized_text': 'vsako leto v evropski uniji pri črpanju iz evropskih skladov izgubljamo velikanske količine sredstev zato zgolj pozivanje k transparentnosti ne bo dovolj če ne bodo temu sledili tudi konkretni ukrepi.', 'gender': 'female', 'speaker_id': '125103', 'is_gold_transcript': True, 'accent': 'None'}]\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:32.361472Z","iopub.execute_input":"2024-11-25T09:19:32.361812Z","iopub.status.idle":"2024-11-25T09:19:32.370107Z","shell.execute_reply.started":"2024-11-25T09:19:32.361780Z","shell.execute_reply":"2024-11-25T09:19:32.368802Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"IterableDataset({\n    features: ['audio_id', 'language', 'audio', 'raw_text', 'normalized_text', 'gender', 'speaker_id', 'is_gold_transcript', 'accent'],\n    num_shards: 1\n})"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"# # Count the length of the original streaming dataset\n# dataset_length = sum(1 for _ in dataset)\n# print(f\"The length of the original dataset is: {dataset_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:32.371734Z","iopub.execute_input":"2024-11-25T09:19:32.372159Z","iopub.status.idle":"2024-11-25T09:19:32.389302Z","shell.execute_reply.started":"2024-11-25T09:19:32.372109Z","shell.execute_reply":"2024-11-25T09:19:32.387683Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Shuffle dataset\ntorch.manual_seed(42)\ndataset = dataset.shuffle(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:32.391068Z","iopub.execute_input":"2024-11-25T09:19:32.391641Z","iopub.status.idle":"2024-11-25T09:19:32.408666Z","shell.execute_reply.started":"2024-11-25T09:19:32.391582Z","shell.execute_reply":"2024-11-25T09:19:32.407513Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Step 4: Feature Extraction Using Wav2Vec2\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\n\n# Load pretrained Wav2Vec2 model and processor\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\nwav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:32.409950Z","iopub.execute_input":"2024-11-25T09:19:32.410313Z","iopub.status.idle":"2024-11-25T09:19:33.416664Z","shell.execute_reply.started":"2024-11-25T09:19:32.410277Z","shell.execute_reply":"2024-11-25T09:19:33.415257Z"}},"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# Function for feature extraction\ndef extract_features(batch):\n    inputs = processor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\", padding=True)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    with torch.no_grad():\n        outputs = wav2vec2_model(**inputs)\n    return {\"features\": outputs.last_hidden_state.cpu().numpy()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:33.418528Z","iopub.execute_input":"2024-11-25T09:19:33.418964Z","iopub.status.idle":"2024-11-25T09:19:33.426713Z","shell.execute_reply.started":"2024-11-25T09:19:33.418925Z","shell.execute_reply":"2024-11-25T09:19:33.425498Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"# Apply feature extraction\ndataset = dataset.map(extract_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:19:33.430918Z","iopub.execute_input":"2024-11-25T09:19:33.431371Z","iopub.status.idle":"2024-11-25T09:19:33.444726Z","shell.execute_reply.started":"2024-11-25T09:19:33.431331Z","shell.execute_reply":"2024-11-25T09:19:33.443496Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"max_samples = 2101\nmin_feature_length = float('inf')\n\nfor i, example in enumerate(dataset):\n    if i >= max_samples:\n        break\n    feature_length = example['features'].shape[1]\n    min_feature_length = min(min_feature_length, feature_length)\n\nprint(f\"Minimum feature length (limited to {max_samples} samples): {min_feature_length}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the maximum feature length\nmax_feature_length = float('-inf')  # Start with a very small value\n\n# Iterate over the streaming dataset\nfor example in dataset:\n    feature_length = example['features'].shape[1]  # Access the feature length\n    max_feature_length = max(max_feature_length, feature_length)  # Update maximum\n\nprint(f\"Maximum feature length: {max_feature_length}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Collect filtered examples into a list\nfiltered_examples = []\nfor example in dataset:\n    if example[\"features\"].shape[1] >= 200:  # Apply filtering condition\n        filtered_examples.append(example)\n\n# Convert the filtered examples into a non-streaming dataset\nfiltered_dataset = Dataset.from_list(filtered_examples)\n\nprint(f\"Length of filtered dataset: {len(filtered_dataset)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = filtered_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the first 5 examples from the dataset to check the data\nfor i in range(5):\n    print(dataset[i]['features'].shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map speakers to integer labels\nspeakers = sorted(set(dataset[\"speaker_id\"]))\nspeaker_to_label = {speaker: idx for idx, speaker in enumerate(speakers)}\ndataset = dataset.map(lambda x: {\"label\": speaker_to_label[x[\"speaker_id\"]]})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Access the speaker ID of the first example in the filtered dataset\nfirst_example_speaker_id = dataset[0][\"speaker_id\"]\nprint(f\"Speaker ID of the first example: {first_example_speaker_id}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the number of unique speakers in the filtered dataset\nunique_speakers = len(set([x[\"speaker_id\"] for x in dataset]))\nprint(f\"Number of unique speakers: {unique_speakers}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Model Definition\nimport torch.nn as nn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(768, 256, kernel_size=3)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.conv2 = nn.Conv1d(256, 128, kernel_size=3)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 32, kernel_size=3)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.fc1 = nn.Linear(32, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool1d(2)\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = self.relu(self.bn3(self.conv3(x)))\n        x = self.pool(x)\n        x = self.global_avg_pool(x).squeeze(-1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Data Splitting and DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset\ntrain_data, test_data = train_test_split(list(dataset), test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the length of the training dataset\ntrain_length = len(train_data)\nprint(f\"Length of the train dataset: {train_length}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the batch size for training\nbatch_size = 100  # As per the instructions\n\n# Calculate the number of batches in the training dataset\nnum_batches_train = (len(train_data) + batch_size - 1) // batch_size  # Ceiling division\nprint(f\"Number of batches in train data: {num_batches_train}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom PyTorch Dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        features = torch.tensor(item[\"features\"][:200], dtype=torch.float32).permute(1, 0)  # Truncate to 200 frames\n        label = torch.tensor(item[\"label\"], dtype=torch.long)\n        return features, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Datasets and DataLoaders\nbatch_sizes = {\"train\": 100, \"val\": 10, \"test\": 1}\ntrain_loader = DataLoader(CustomDataset(train_data), batch_size=batch_sizes[\"train\"], shuffle=True)\nval_loader = DataLoader(CustomDataset(val_data), batch_size=batch_sizes[\"val\"], shuffle=False)\ntest_loader = DataLoader(CustomDataset(test_data), batch_size=batch_sizes[\"test\"], shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Training the Model\nmodel = CNNModel(num_classes=len(speakers)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nfor epoch in range(100):\n    model.train()\n    train_loss = 0\n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for features, labels in val_loader:\n            features, labels = features.to(device), labels.to(device)\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Evaluating on the Test Set\nfrom sklearn.metrics import accuracy_score\n\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for features, labels in test_loader:\n        features, labels = features.to(device), labels.to(device)\n        outputs = model(features)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\ntest_accuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}